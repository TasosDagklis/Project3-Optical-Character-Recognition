# -*- coding: utf-8 -*-
"""OpticalCharacterRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rR0mUGIFIf1Iy8Grfs4yA7k7qik4g4rt
"""

import numpy as np
np.set_printoptions(precision=3)
from IPython.display import Markdown
import matplotlib.pyplot as plt

import csv
results = []
with open('letter-recognition.csv') as csvfile:
  data = csv.reader(csvfile, delimiter=',')
  for dataline in data:
    results.append(dataline)

from sklearn.preprocessing import LabelEncoder
#coverting the results list to array
data = np.array(results)

#slicing the data into X and y
X_str = data[:, 1:]
y_str = data[:, 0]

#converting the X array from string to float
X = X_str.astype(float)
le = LabelEncoder().fit(y_str)
y = le.transform(y_str)

#split the data into training set and test set
X_train = X[:16000, :16000]
X_test = X[16000:, :]
y_train = y[:16000]
y_test = y[16000:]

#Print the mean, std, max, min of X_train and X_test
display(Markdown(
    "###Characteristics of the X training set:\n" +
    "**Max**: " + str(np.max(X_train)) + "<br>" +
    "**Min**: " + str(np.min(X_train))
    ))
display(Markdown("**Mean feature values:**"))
print(np.mean(X_train, axis=0))
display(Markdown("**Std of feature values:**"))
print(np.std(X_train, axis=0))

display(Markdown(
    "###Characteristics of the X testing set:\n" +
    "**Max**: " + str(np.max(X_test)) + "<br>" +
    "**Min**: " + str(np.min(X_test))
    ))
display(Markdown("**Mean feature values:**"))
print(np.mean(X_test, axis=0))
display(Markdown("**Std of feature values:**"))
print(np.std(X_test, axis=0))

"""# **Logistic Regression**"""

#Classification with Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler

log_reg_pipeline=Pipeline([('Scaler',  MinMaxScaler()), ('LogRegClassifier', LogisticRegression(max_iter=1000))])

params = { 
           'LogRegClassifier__C': np.logspace(-2, 3, 6) # try 6 values in logarithmic scale from 10^-2 to 10^3
         }

grid = GridSearchCV(estimator=log_reg_pipeline, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train,y_train)

y_train_pred = grid.predict(X_train)
y_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα λογιστικής παλινδρόμησης:\n" +
    "**Best lambda**: {:.3f}".format(grid.best_params_['LogRegClassifier__C'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **Linear SVM**"""

#Classification with linear SVM
from sklearn.svm import LinearSVC

svm_pipeline=Pipeline([('Scaler',  MinMaxScaler()), ('SVMClassifier', LinearSVC())])

params = { 
              'SVMClassifier__C': np.logspace(0, 3, 4) # try 4 values in logarithmic scale from 10^0 to 10^3
         }

grid = GridSearchCV(estimator=svm_pipeline, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train,y_train)

y_train_pred = grid.predict(X_train)
y_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα SVM:\n" +
    "**Best C**: {:.3f}".format(grid.best_params_['SVMClassifier__C'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **K-Nearest Neighbours**"""

#Non linear classification with k-Nearest Neighbours
from sklearn.neighbors import KNeighborsClassifier

kNN_pipeline=Pipeline([('Scaler',  MinMaxScaler()), ('kNNClassifier', KNeighborsClassifier())])

params = { 
           'kNNClassifier__n_neighbors':   range(1,7),
           'kNNClassifier__weights':   ['uniform', 'distance']
         }

grid = GridSearchCV(estimator=kNN_pipeline, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train, y_train) 

y_train_pred = grid.predict(X_train)
y_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα k-nearest neighbors classification:\n" +
    "**Best k**: {:.3f}".format(grid.best_params_['kNNClassifier__n_neighbors'])  + "<br>" +
    "**Best weighting**: {}".format(grid.best_params_['kNNClassifier__weights'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **SVM RBF**"""

#Classification with SVM RBF
from sklearn.svm import SVC

rbf_pipeline=Pipeline([('Scaler',  MinMaxScaler()), ('SVCClassifier', SVC())])

params = { 
              'SVCClassifier__C': np.logspace(0, 3, 4), # try 4 values in logarithmic scale from 10^0 to 10^3
              'SVCClassifier__gamma': np.logspace(-1, 1, 6), #try 6 values in logarithmic scale from 10^-1 to 10^1
         }

grid = GridSearchCV(estimator=rbf_pipeline, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train,y_train)

rbf_train_pred = grid.predict(X_train)
rbf_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα SVM RBF:\n" +
    "**Best C**: {:.3f}".format(grid.best_params_['SVCClassifier__C'])  + "<br>" +
    "**Best Gamma**: {:.3f}".format(grid.best_params_['SVCClassifier__gamma'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, rbf_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, rbf_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, rbf_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, rbf_test_pred,average='weighted'))
))

"""# **Random Forest**"""

#Classification with Random Forests
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(max_leaf_nodes=100, random_state=0)

forest.fit(X_train, y_train)
y_train_pred = forest.predict(X_train)
y_test_pred = forest.predict(X_test)

display(Markdown(
    "### Αποτελέσματα Random Forest classification without grid search:\n" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **Random Forest with Grid Search**"""

#Grid Search for the random forests
randomforest_pipe=Pipeline([('Scaler',  MinMaxScaler()), ('RandomForestClassifier', RandomForestClassifier())])

params = { 
            'RandomForestClassifier__min_samples_leaf':  range(90,100),
            'RandomForestClassifier__max_leaf_nodes'  :  range(90,100)
         }

grid = GridSearchCV(estimator=randomforest_pipe, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train, y_train) 

y_train_pred = grid.predict(X_train)
y_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα Random Forest classification:\n" +
    "**Best min sample leaf**: {:.3f}".format(grid.best_params_['RandomForestClassifier__min_samples_leaf'])  + "<br>" +
    "**Best max leaf node**: {}".format(grid.best_params_['RandomForestClassifier__max_leaf_nodes'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **Extremely Randomized Trees**"""

#Classification with extremely randomized trees
from sklearn.ensemble import ExtraTreesClassifier


extratree = ExtraTreesClassifier(max_leaf_nodes=100, random_state=0)

extratree.fit(X_train, y_train)
y_train_pred = extratree.predict(X_train)
y_test_pred = extratree.predict(X_test)

display(Markdown(
    "### Αποτελέσματα Extremely Randomized Trees classification without grid search:\n" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **Extremely Randomized Trees with Grid Search**"""

#Grid Search for the extremily randomized trees
extratrees_pipeline=Pipeline([('Scaler',  MinMaxScaler()), ('ExtraTreesClassifier', ExtraTreesClassifier())])

params = { 
            'ExtraTreesClassifier__min_samples_leaf':  range(90,100),
            'ExtraTreesClassifier__max_leaf_nodes'  :  range(90,100)
         }

grid = GridSearchCV(estimator=extratrees_pipeline, cv = 5, param_grid=params, verbose=4)
grid.fit(X_train, y_train) 

y_train_pred = grid.predict(X_train)
y_test_pred = grid.predict(X_test)

display(Markdown(
    "### Αποτελέσματα Extremily Randomized Trees classification:\n" +
    "**Best min sample leaf**: {:.3f}".format(grid.best_params_['ExtraTreesClassifier__min_samples_leaf'])  + "<br>" +
    "**Best max leaf node**: {}".format(grid.best_params_['ExtraTreesClassifier__max_leaf_nodes'])  + "<br>" +
    "**Best Cross-Val Accuracy**: {:.2f}".format(grid.best_score_) + "<br>" +
    "**Test Set Accuracy**: {:.2f}".format(accuracy_score(y_test, y_test_pred)) + "<br>" +
    "**Test Set Precision**: {:.2f}".format(precision_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set Recall**: {:.2f}".format(recall_score(y_test, y_test_pred, average='weighted')) + "<br>" +
    "**Test Set F1**: {:.2f}".format(f1_score(y_test, y_test_pred,average='weighted'))
))

"""# **Classification Report for the best algorithm**"""

#Print the classification report of the best algorithm
from sklearn.metrics import classification_report
print(classification_report(y_test, rbf_test_pred))

"""Με βάση το precision στο classification report δύο κλάσεις ταξινομούνται χειρότερα από τις υπόλοιπες και ανήκουν στους χαρακτήρες B και H με precision=0.94

# **Συμπεράσματα**
Βλέποντας όλους τους αλγορίθμους παρατηρώ πως οι αλγόριθμοι γραμμικής ταξινόμησης δεν αποδίδουν τόσο αποτελεσματικά όσο οι αλγόριθμοι μη γραμμικής ταξινόμησης. Συγκεκριμένα, η λογιστική παλινδρόμηση καθώς και ο SVM δίνουν χαμηλά αποτελέσματα, ωστόσο, δεν απαιτούν πολύ χρόνο για να τρέξουν. Από την άλλη, οι αλγόριθμοι μη γραμμικής ταξινόμησης δίνουν πολύ καλύτερα αποτελέσματα. Βλέπουμε βελτίωση στα αποτελέσματα με την εφαρμογή του kNN μοντέλου καθώς επίσης και λιγότερο χρόνο εκτέλεσης. Ο αλγόριθμος SVM πυρήνα ακτινικής βάσης δίνει τα καλύτερα αποτελέσματα από όλα τα μοντέλα καθώς χρησιμοποιεί το kernel-trick το οποίο δεδομένου του μεγάλου αριθμού δεδομένων αυξάνει τις πιθανότητες για καλύτερα αποτελέσματα. Παρ'όλα αυτά όμως, απαιτεί αρκετό χρόνο εκτέλεσης. Τέλος, όσο αναφορά τα Random Forest και Extremily Randomized trees τα αποτελέσματα δεν είναι τόσο κάλα όσο ήταν στο kNN και SVM RBF. Όσο πιο μεγάλο το βάθος τόσο καλύτερα αποτελέσματα μας δίνει. Επίσης, φαίνεται πως η χρήση υπερπαραμέτρων δεν μας δίνει κάποια βελτίωση, αντιθέτως, βλέπουμε επιδείνωση.
"""